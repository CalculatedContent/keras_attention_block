
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>keras_attention_block.self_attention &#8212; keras_attention_block  documentation</title>
    <link rel="stylesheet" href="../../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../',
        VERSION:     '',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
   
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for keras_attention_block.self_attention</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">collections.abc</span> <span class="k">import</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Sequence</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="k">import</span> <span class="n">backend</span> <span class="k">as</span> <span class="n">K</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="k">import</span> <span class="n">initializers</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="k">import</span> <span class="n">activations</span>
<span class="kn">from</span> <span class="nn">keras.engine.topology</span> <span class="k">import</span> <span class="n">Layer</span>


<div class="viewcode-block" id="SelfAttention1DLayer"><a class="viewcode-back" href="../../keras_attention_block.html#keras_attention_block.self_attention.SelfAttention1DLayer">[docs]</a><span class="k">class</span> <span class="nc">SelfAttention1DLayer</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;self-attention的特点是自己为输入,输出也是一个和自己一样shape的张量.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        similarity (Union[Callable,str]): - 指定使用的相似度计算函数,目前可选的有\</span>
<span class="sd">        加性相似度(additive),乘性相似度(multiplicative),点乘相似度(dot_product),\</span>
<span class="sd">        当然也可以自己写一个,只要最终输出的是一个(?,output,input_timestep)的</span>
<span class="sd">        kernel_size (tuple[int,int]): - 指定使用加性相似度(additive)时才能指定,\</span>
<span class="sd">        用于指定第一个权重的形状,各维的意义[输出的纬度,第二个权重矩阵的第一个纬度]</span>
<span class="sd">        kernel_initializer (str): - 第一个权重的初始化函数,默认glorot_uniform</span>
<span class="sd">        wk_kernel_initializer (str): - 第二个权重的初始化函数,默认glorot_uniform</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">similarity</span><span class="o">=</span><span class="s2">&quot;additive&quot;</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span>
                 <span class="n">kernel_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">kernel_initializer</span><span class="o">=</span><span class="s1">&#39;glorot_uniform&#39;</span><span class="p">,</span>
                 <span class="n">wk_kernel_initializer</span><span class="o">=</span><span class="s1">&#39;glorot_uniform&#39;</span><span class="p">,</span>
                 <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">similarity</span><span class="p">,</span> <span class="n">Callable</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">similarity</span> <span class="o">=</span> <span class="n">similarity</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">similarity</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">and</span> <span class="n">similarity</span> <span class="ow">in</span> <span class="p">(</span>
                <span class="s2">&quot;multiplicative&quot;</span><span class="p">,</span> <span class="s2">&quot;dot_product&quot;</span><span class="p">,</span> <span class="s2">&quot;additive&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">similarity</span> <span class="o">=</span> <span class="n">similarity</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s1">&#39;similarity now only support &#39;</span>
                <span class="s1">&#39;&quot;multiplicative&quot;,&quot;dot_product&quot;,&quot;additive&quot;,&#39;</span>
                <span class="s1">&#39;and you can input a function as the similarity function!&#39;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">similarity</span> <span class="o">==</span> <span class="s2">&quot;additive&quot;</span> <span class="ow">and</span> <span class="n">kernel_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s1">&#39;additive similarity need &#39;</span>
                <span class="s1">&#39;hyperparameter kernel_size!&#39;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">similarity</span> <span class="o">!=</span> <span class="s2">&quot;additive&quot;</span> <span class="ow">and</span> <span class="n">kernel_size</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s1">&#39;only additive similarity support &#39;</span>
                <span class="s1">&#39;hyperparameter kernel_size!&#39;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span>
            <span class="n">kernel_size</span><span class="p">,</span>
                <span class="n">Sequence</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">)</span> <span class="ow">or</span> <span class="n">kernel_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">kernel_size</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s1">&#39;kernel_size must be a Sequence with 2 int element&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_initializer</span> <span class="o">=</span> <span class="n">initializers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">kernel_initializer</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wk_kernel_initializer</span> <span class="o">=</span> <span class="n">initializers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="n">wk_kernel_initializer</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">similarity</span> <span class="o">==</span> <span class="s2">&quot;additive&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">wk_kernel</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">wq_kernel</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">similarity</span> <span class="o">==</span> <span class="s2">&quot;multiplicative&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_build_w</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">time</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">similarity</span> <span class="o">==</span> <span class="s2">&quot;additive&quot;</span><span class="p">:</span>
            <span class="n">r</span><span class="p">,</span> <span class="n">d_a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;kernel&#39;</span><span class="p">,</span>
                                          <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">d_a</span><span class="p">),</span>
                                          <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_initializer</span><span class="p">,</span>
                                          <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">wk_kernel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;wk_kernel&#39;</span><span class="p">,</span>
                <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">d_a</span><span class="p">,</span> <span class="n">dim</span><span class="p">),</span>
                <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">wk_kernel_initializer</span><span class="p">,</span>
                <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">similarity</span> <span class="o">==</span> <span class="s2">&quot;multiplicative&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;kernel&#39;</span><span class="p">,</span>
                                          <span class="n">shape</span><span class="o">=</span><span class="p">(</span>
                                              <span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">),</span>
                                          <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_initializer</span><span class="p">,</span>
                                          <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>

<div class="viewcode-block" id="SelfAttention1DLayer.build"><a class="viewcode-back" href="../../keras_attention_block.html#keras_attention_block.self_attention.SelfAttention1DLayer.build">[docs]</a>    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;A additive weight layer should be called &#39;</span>
                             <span class="s1">&#39;by a (batch,time_step,dim)3D inputs.&#39;</span>
                             <span class="s1">&#39;Got &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39; inputs.&#39;</span><span class="p">)</span>

        <span class="n">time</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_build_w</span><span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
        <span class="c1"># Be sure to call this somewhere!</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span></div>

<div class="viewcode-block" id="SelfAttention1DLayer.multiplicative"><a class="viewcode-back" href="../../keras_attention_block.html#keras_attention_block.self_attention.SelfAttention1DLayer.multiplicative">[docs]</a>    <span class="k">def</span> <span class="nf">multiplicative</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Source</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;乘性相似度,其中的权重矩阵形状为[dim,dim]\</span>
<span class="sd">        输出的固定为与原输入一样形状</span>

<span class="sd">        .. math::  Similarity(Source) =  Source\cdot W \cdot Source^T</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">Source_t</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">permute_dimensions</span><span class="p">(</span><span class="n">Source</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Source</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">)</span>
        <span class="n">sim</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">batch_dot</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">Source_t</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">sim</span></div>

<div class="viewcode-block" id="SelfAttention1DLayer.dot_product"><a class="viewcode-back" href="../../keras_attention_block.html#keras_attention_block.self_attention.SelfAttention1DLayer.dot_product">[docs]</a>    <span class="k">def</span> <span class="nf">dot_product</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Source</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;点乘相似度,在google的attention is all you need 中看到的.\</span>
<span class="sd">        很迷,没有要训练的矩阵,直接转置点乘,输出的固定为与原输入一样形状</span>

<span class="sd">        .. math::  Similarity(Source) = \frac{Source^T\cdot Source}{\sqrt{d_k}}</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">Source_t</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">permute_dimensions</span><span class="p">(</span><span class="n">Source</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">sim</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">batch_dot</span><span class="p">(</span><span class="n">Source</span><span class="p">,</span> <span class="n">Source_t</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">sim</span></div>

<div class="viewcode-block" id="SelfAttention1DLayer.additive"><a class="viewcode-back" href="../../keras_attention_block.html#keras_attention_block.self_attention.SelfAttention1DLayer.additive">[docs]</a>    <span class="k">def</span> <span class="nf">additive</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Source</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        加性相似度,最经典的注意力相似度机制,如果是在self attention中\</span>
<span class="sd">        则该层有两个权重矩阵形状为(r,d_a)和(d_a,dim)</span>

<span class="sd">        .. math::  Similarity(Source) = V \cdot tanh(W_k\cdot Source^T)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">Source_t</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">permute_dimensions</span><span class="p">(</span><span class="n">Source</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">f_att</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wk_kernel</span><span class="p">,</span> <span class="n">Source_t</span><span class="p">)</span>
        <span class="n">f_att</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">permute_dimensions</span><span class="p">(</span><span class="n">f_att</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">sim</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span> <span class="n">K</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">f_att</span><span class="p">))</span>
        <span class="n">sim</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">permute_dimensions</span><span class="p">(</span><span class="n">sim</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">sim</span></div>

    <span class="k">def</span> <span class="nf">_call_attention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Source</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;self-attention就是通过相似度函数计算得的相似矩阵过softmax后与自身点乘得到</span>

<span class="sd">        .. math::  A = Softmax(Similarity(Source))</span>
<span class="sd">        .. math::  C = A \cdot Source</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">similarity</span><span class="p">,</span> <span class="n">Callable</span><span class="p">):</span>
            <span class="n">sim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="n">Source</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">sim</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">similarity</span><span class="p">)(</span><span class="n">Source</span><span class="p">)</span>
        <span class="n">sm</span> <span class="o">=</span> <span class="n">activations</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">sim</span><span class="p">)</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">batch_dot</span><span class="p">(</span><span class="n">sm</span><span class="p">,</span> <span class="n">Source</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">result</span>

<div class="viewcode-block" id="SelfAttention1DLayer.call"><a class="viewcode-back" href="../../keras_attention_block.html#keras_attention_block.self_attention.SelfAttention1DLayer.call">[docs]</a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">Source</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_attention</span><span class="p">(</span><span class="n">Source</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">result</span></div>

<div class="viewcode-block" id="SelfAttention1DLayer.compute_output_shape"><a class="viewcode-back" href="../../keras_attention_block.html#keras_attention_block.self_attention.SelfAttention1DLayer.compute_output_shape">[docs]</a>    <span class="k">def</span> <span class="nf">compute_output_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">)</span></div>

<div class="viewcode-block" id="SelfAttention1DLayer.get_config"><a class="viewcode-back" href="../../keras_attention_block.html#keras_attention_block.self_attention.SelfAttention1DLayer.get_config">[docs]</a>    <span class="k">def</span> <span class="nf">get_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;similarity&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">similarity</span><span class="p">,</span>
            <span class="s1">&#39;kernel_size&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span>
            <span class="s1">&#39;kernel_initializer&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_initializer</span><span class="p">,</span>
            <span class="s1">&#39;wk_kernel_initializer&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">wk_kernel_initializer</span>
        <span class="p">}</span>
        <span class="n">base_config</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>
        <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">base_config</span><span class="o">.</span><span class="n">items</span><span class="p">())</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">items</span><span class="p">()))</span></div></div>


<div class="viewcode-block" id="SelfAttention2DLayer"><a class="viewcode-back" href="../../keras_attention_block.html#keras_attention_block.self_attention.SelfAttention2DLayer">[docs]</a><span class="k">class</span> <span class="nc">SelfAttention2DLayer</span><span class="p">(</span><span class="n">SelfAttention1DLayer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;self-attention的特点是自己为输入,输出也是一个和自己一样shape的张量.</span>
<span class="sd">    2d的self-attention是为2dcnn设计的,其原理就是将4维的中间两维压缩为一维,之后输出的时候再解压缩出来.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        output_size (tuple[int,int]): - 指定输出的形状,如果是加性相似度(additive),\</span>
<span class="sd">        则必须指定,且可以随意指定,如果是其他则可以不指定,那就是原样形状输出,\</span>
<span class="sd">        如果指定的话则必须积与原形状第1,2纬度的积相等</span>
<span class="sd">        similarity (Union[Callable,str]): - 指定使用的相似度计算函数,目前可选的有\</span>
<span class="sd">        加性相似度(additive),乘性相似度(multiplicative),点乘相似度(dot_product),\</span>
<span class="sd">        当然也可以自己写一个,只要最终输出的是一个(?,output,input_timestep)的\</span>
<span class="sd">        用于指定第一个权重的形状,各维的意义[输出的纬度,第二个权重矩阵的第一个纬度]</span>
<span class="sd">        d_a (int): - 使用加性相似度(additive)时才要指定,超参,用于指定第二个权重的第一维</span>
<span class="sd">        kernel_initializer (str): - 第一个权重的初始化函数,默认glorot_uniform</span>
<span class="sd">        wk_kernel_initializer (str): - 第二个权重的初始化函数,默认glorot_uniform</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">similarity</span><span class="o">=</span><span class="s2">&quot;additive&quot;</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span>
                 <span class="n">d_a</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">kernel_initializer</span><span class="o">=</span><span class="s1">&#39;glorot_uniform&#39;</span><span class="p">,</span>
                 <span class="n">wk_kernel_initializer</span><span class="o">=</span><span class="s1">&#39;glorot_uniform&#39;</span><span class="p">,</span>
                 <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="n">output_size</span>
        <span class="k">if</span> <span class="n">similarity</span> <span class="o">==</span> <span class="s2">&quot;additive&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">d_a</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">output_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span>
                    <span class="n">output_size</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">kernel_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">output_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">output_size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">d_a</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s1">&#39;additive similarity need  hyperparameter d_a,output_size&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">kernel_size</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">similarity</span><span class="o">=</span><span class="n">similarity</span><span class="p">,</span>
                         <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
                         <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">kernel_initializer</span><span class="p">,</span>
                         <span class="n">wk_kernel_initializer</span><span class="o">=</span><span class="n">wk_kernel_initializer</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<div class="viewcode-block" id="SelfAttention2DLayer.build"><a class="viewcode-back" href="../../keras_attention_block.html#keras_attention_block.self_attention.SelfAttention2DLayer.build">[docs]</a>    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">4</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;A additive weight layer should be called &#39;</span>
                             <span class="s1">&#39;by a (batch,time_step,dim)3D inputs.&#39;</span>
                             <span class="s1">&#39;Got &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39; inputs.&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">similarity</span> <span class="o">!=</span> <span class="s2">&quot;additive&quot;</span> <span class="ow">and</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span>
                <span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span> <span class="o">!=</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">[</span><span class="mi">1</span><span class="p">])):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;output size error &#39;</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
        <span class="n">time</span> <span class="o">=</span> <span class="n">X</span> <span class="o">*</span> <span class="n">Y</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_build_w</span><span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
        <span class="c1"># Be sure to call this somewhere!</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SelfAttention1DLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span></div>

<div class="viewcode-block" id="SelfAttention2DLayer.call"><a class="viewcode-back" href="../../keras_attention_block.html#keras_attention_block.self_attention.SelfAttention2DLayer.call">[docs]</a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;self-attention就是通过相似度函数计算得的相似矩阵过softmax后与自身点乘得到</span>

<span class="sd">        .. math::  A = Softmax(Similarity(Source))</span>
<span class="sd">        .. math::  C = A \cdot Source</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">input_shape</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span>
        <span class="c1"># help(input_shape[1])</span>
        <span class="n">Source</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span>
                <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                    <span class="n">input_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">value</span><span class="p">]))</span>
        <span class="n">_result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_attention</span><span class="p">(</span><span class="n">Source</span><span class="p">)</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">_result</span><span class="p">,</span>
            <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">]))</span>
        <span class="k">return</span> <span class="n">result</span></div>

<div class="viewcode-block" id="SelfAttention2DLayer.compute_output_shape"><a class="viewcode-back" href="../../keras_attention_block.html#keras_attention_block.self_attention.SelfAttention2DLayer.compute_output_shape">[docs]</a>    <span class="k">def</span> <span class="nf">compute_output_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>  <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">)</span></div>

<div class="viewcode-block" id="SelfAttention2DLayer.get_config"><a class="viewcode-back" href="../../keras_attention_block.html#keras_attention_block.self_attention.SelfAttention2DLayer.get_config">[docs]</a>    <span class="k">def</span> <span class="nf">get_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;output_size&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span>
        <span class="p">}</span>
        <span class="n">base_config</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>
        <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">base_config</span><span class="o">.</span><span class="n">items</span><span class="p">())</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">items</span><span class="p">()))</span></div></div>


<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;SelfAttention1DLayer&quot;</span><span class="p">,</span> <span class="s2">&quot;SelfAttention2DLayer&quot;</span><span class="p">]</span>
</pre></div>

          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper"><div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../index.html">Documentation overview</a><ul>
  <li><a href="../index.html">Module code</a><ul>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../../search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2017, hsz.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.6.5</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.10</a>
      
    </div>

    

    
  </body>
</html>